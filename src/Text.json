{
  "text": [
    "It is known that electronic devices such as computers and smartphones are getting faster through the time, also their storage capacity is being increased. According to the Moore’s Law, the density of transistors on microchips is increasing at the rate of being twice every two years, making processors faster. Nowadays, almost all the hardware limitations there were years ago have disappeared. In consequence, caring about performance is getting lower importance for most of developers. It got usual to think is enough the software solution works and does what is supposed to do. Then, the performance is not considered truly relevant anymore.",
    "However, performance still being as important as used to be when devices had more hardware limitations; even is possible to say it has got more importance since the technological requirements have increased too. A slightly slower implementation in a small project that can be ignored due to its seemingly insignificant difference compared to the optimal one, can easily scale to a huge delay when the project complexity increase. It takes relevance when it is realized that a response time difference of only one second can make the difference between keep users and lose them.",
    "Now, the question is: how can we measure the performance of an algorithm? To answer this question, we need to know that running any block of code, no matter the language is written in, and the device is run on, always consumes certain amount of time and memory; then, it is possible to measure the efficiency of the algorithm in terms of these two metrics. Nevertheless, the time and memory consumption can fluctuate from one device to another and even between executions. So, in order to make those evaluations in a way that avoid those fluctuations, we can get the number of steps or iterations needed to complete the algorithm in terms of n, which is the amount of data is used as input by the algorithm; the same approach is used to determine how much pieces of information are needed to execute the algorithm and get the output.",
    "Big-O notation provides an easy way to understand the behavior of an algorithm in terms of the size of the input data, given as a result a simple non-decreasing mathematical function that shows the time (steps or iterations) and the memory (pieces of information) used by an algorithm to give some result for each amount of input data. There are several “families” of these functions that fits very well the behavior of almost every algorithm, ordered to greater to less efficiency: constant, linear, logarithmic, polynomic, exponential, and factorial functions.",
    "An useful example, based on a very frequent task, of the importance of programming an optimal algorithm is sorting a set of elements. There are many ways to achieve this goal, each of them with different need of time and memory consumption. If two of them, the bubble sort, and the merge sort, are compared, is noticed two things: the first one is that the bubble sort is really much easier to implement than the merge one; but, the second and most important one is the behavior of the time taken by both algorithms when the amount of data grows.",
    "The bubble sort has an average sorting performance, following the Big-O notation (in terms of time) of O(n2), while the merge sort takes only a time of O(n log n). Even if it does not mean too much with small sets of data, such as 10 elements; with sets of many elements, such as 10,000 elements, the difference is abysmal. In the first case, with the bubble sort it takes 100 steps in average to complete the task, while with the merge sort takes only 10 steps. If ten times faster is not enough with only 10 items, with 10,000 items the running time is of 100,000,000 steps versus only 40,000; in few words, the time consumption of the merge sort algorithm, relative to the bubble sort one, became 2,500 times faster.",
    "Another key and also frequently ignored factor than can cause bottlenecks in the performance of a software solution is the proper use of data structures. A data structure can be seen as simple as the way a set of data is stored; however, it affects completely how more data is added to the set, how it is accessed, how it is updated and deleted. Each data structure has its own time complexity (remember Big-O notation) to each mentioned operation: add, access, update and delete; also, different data structures are optimal in different operations and non-optimal in the other ones. Then, choosing the most suitable data structure for a given case is crucial to ensure the operations in the set of data that are more frequent be optimal to have a better average performance over the whole operations made in the set.",
    "For instance, two big known data structures are Array and LinkedList, both of them can store an ordered set of items. However, access data in an array is O(1) while the other ones are O(n); by the other hand, a linked-list has a time complexity to add data of O(1) while the other ones are O(n) in average. If we have a frequent accessed set, an array is suggested; but if the data is frequently added at the beginning or the end, then a linked-list is suggested instead. If you are now familiar with time complexity and Big-O notation, you will notice that it can make differences as huge as n times faster or slower depending on if the right data structure is whether chosen.",
    "In conclusion, there are several things that can affect the performance of a software solution or algorithm. In this occasion two of them were covered: time and memory complexity of the algorithm, and data structures. Also, it was seen that going for the easier way can costs too much to the software efficiency. Furthermore, remember that the optimal implementation can scale more smoothly and give results thousands better. Do not forget that development is more than simply make the code works."
  ]
}